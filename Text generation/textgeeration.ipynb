{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ur1R8F5O8im",
        "outputId": "0eb6f8d5-0957-4e55-8ae0-fecbc5fb35a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "sPmdJsK7PT5V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000]#part of data"
      ],
      "metadata": {
        "id": "C6nxqSxgPqvp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text))) #filter out all of unique charchters and sorting them like abcd df\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars)) #enumarte them till the max convert them to numerical char is key index is value\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars)) #from numerical to char again"
      ],
      "metadata": {
        "id": "pG9_05LoPugH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40 #40 char predict next char\n",
        "STEP_SIZE = 3 #how many char are going to shift\n",
        "sentences = [] #features\n",
        "next_chars = [] #target"
      ],
      "metadata": {
        "id": "L5YDBKoQQ6v7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE): #loops from beinign of text to last part of text\n",
        "    sentences.append(text[i: i + SEQ_LENGTH]) #5 0 to 4\n",
        "    next_chars.append(text[i + SEQ_LENGTH]) #5 is next char"
      ],
      "metadata": {
        "id": "nGS-zqnBQNIm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert data to numpy array\n",
        "x=np.zeros((len(sentences), SEQ_LENGTH, len(chars)), dtype=bool)\n",
        "y=np.zeros((len(sentences), len(chars)), dtype=bool)"
      ],
      "metadata": {
        "id": "5cVzhUT6QM9m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i , sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1 #sentence num i at position num t and char num whatever this position is set to 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1 #target data same thing"
      ],
      "metadata": {
        "id": "q-LUQ59jQM2x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(chars))))\n",
        "model.add(Dense(len(chars))) #if we have 10 char then it is layer of 10 neurons\n",
        "model.add(Activation('softmax')) #scale output all values scale to one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-pSs3pQMw-",
        "outputId": "af8a28b0-d014-4370-f1e8-dc63675636fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))"
      ],
      "metadata": {
        "id": "bu09AkIbRu89"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x, y, batch_size=256, epochs=4) #batch size how many example we willl put into the network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GntknixRu6O",
        "outputId": "3705ee11-d288-4646-b7ed-2cddf6562500"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 179ms/step - loss: 2.4701\n",
            "Epoch 2/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 183ms/step - loss: 1.7755\n",
            "Epoch 3/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 170ms/step - loss: 1.6083\n",
            "Epoch 4/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 177ms/step - loss: 1.5274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acc4fa66740>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0): #temperature predictions of our model and picks one char depend on temp pick is consevative or expermintal\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "LURCfbQOVauJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length): #produce as many char\n",
        "        x = np.zeros((1, SEQ_LENGTH, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t, char_indices[char]] = 1 #at certain position the char conert to index\n",
        "\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample(predictions, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "metadata": {
        "id": "I0jF2E8sVaqM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(300, 0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqlL4VjQVana",
        "outputId": "4ef74f04-f5d3-44d5-c6f5-0c37da32896c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the greatest, able to do least,\n",
            "yet most the fair stand the strength the brother hath strong.\n",
            "\n",
            "romeo:\n",
            "at thou say the hands the father that stand\n",
            "that i seem the marriage that the brother this dear the strength my servest.\n",
            "\n",
            "clown:\n",
            "i so that hast the fair the father the strength the stread the strength the strength the chill stand.\n",
            "\n",
            "king \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(300, 0.6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1aJRur6Vakg",
        "outputId": "89a9ab79-b0f1-4252-8378-71ee438003d9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'tis a derivative from me to mine,\n",
            "and of warwick of the sacross that bear the chilt.\n",
            "\n",
            "romeo:\n",
            "archang warman a first this sander the most what\n",
            "i have seem the worst at out.\n",
            "the brother would side she haster doth a drows of this and counter's both the defence\n",
            "it the traze is what not i have been in the thing?\n",
            "\n",
            "warwick:\n",
            "o'er lie lament the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(300, 0.8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDQK5K6JX7gE",
        "outputId": "6dc9d457-3040-4ffe-b3d5-e46c0b93e6e9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " unborn and unbegot,\n",
            "that lift your vass i may gentlems as stand\n",
            "with his goodle eccursains him before not,\n",
            "this armion.\n",
            "\n",
            "henry bolingboung:\n",
            "what welt they shall not perour again,\n",
            "berouden fage of thron and the stranger\n",
            "the brothe show in little offeress\n",
            "to fain that fair countres, myselfood\n",
            "as the stres and sting storld their serrecty\n",
            "th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zT2jg7SEX7bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6KQIg3JX7XZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}